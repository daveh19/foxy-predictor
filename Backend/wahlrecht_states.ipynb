{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def janitor(table):\n",
    "    \"\"\"\n",
    "    cleans a single table extracted for the states\n",
    "    \"\"\"\n",
    "    # drop rows containing only NaNs/None\n",
    "    table = table.dropna(axis=0, how='all')\n",
    "\n",
    "    # replace all empty entries with NaN\n",
    "    table = table.replace('', 'NaN', regex=True)\n",
    "\n",
    "    table.index = np.arange(table.shape[0])\n",
    "\n",
    "    # drop the rows containing the true results of the elections\n",
    "    Idx = np.where(table['Institut(Datum)'].str.contains('Bundestagswahl'))\n",
    "    table = table.drop(Idx[0])\n",
    "    table.index = np.arange(table.shape[0])\n",
    "\n",
    "    # split 'BefrateZeitraum' into two columns\n",
    "    for i, n in enumerate(table['BefragteZeitraum']):\n",
    "        if pd.isnull(n):\n",
    "            table['BefragteZeitraum'][i] = np.nan\n",
    "        elif len(n) >= 13:\n",
    "            n = n.split('\\n', 1)[0]\n",
    "            table['BefragteZeitraum'][i] = n[:-13]\n",
    "    table.rename(columns={'BefragteZeitraum': 'Befragte'}, inplace=True)\n",
    "    table.Befragte = table.Befragte.str.replace('.','')\n",
    "\n",
    "    # split the column 'Institut(Datum)' into two columnbs\n",
    "    institut_datum = table['Institut(Datum)'].str.extract('([A-z]+)?([(])?(\\d+.\\d+.\\d+)', expand=False)\n",
    "    institut_datum = institut_datum.drop(1, axis=1)\n",
    "    institut_datum.columns = ['Institut', 'Datum']\n",
    "    table = pd.concat([institut_datum, table.iloc[:,1:]], axis=1)\n",
    "\n",
    "    # convert the date to type date\n",
    "    table.Datum = table.Datum.apply(lambda cell: pd.to_datetime(cell, format='%d.%m.%Y')\n",
    "                                      if len(cell)==10 \n",
    "                                      else pd.to_datetime(cell, format='%d.%m.%y'))\n",
    "    if not table.empty:\n",
    "        table.Datum = table.Datum.dt.date\n",
    "\n",
    "    # replace the strings %,-\n",
    "    table = table.replace(',', '.', regex=True)\n",
    "    table = table.replace('[–?%)≈/*]', '', regex=True)\n",
    "    table = table.replace('T • ', '', regex=True)\n",
    "    table = table.replace('O • ', '', regex=True)\n",
    "\n",
    "    # extract the AfD from Sonstige\n",
    "    AfD = table.Sonstige.str.extract('(AfD \\d+)', expand=False)\n",
    "    AfD = AfD.str.extract('(\\d+)', expand=False)\n",
    "    table.insert(9, 'AfD', AfD)\n",
    "\n",
    "    # combine remaining percentages in Sonstige\n",
    "    Sonst = table.Sonstige.replace('(AfD \\d+)', '', regex=True)\n",
    "    Sonst = Sonst.str.findall('(\\d+)')\n",
    "    table.Sonstige = Sonst.apply(lambda cell: np.array(cell).astype(float).sum() if cell!=[] else np.nan)\n",
    "\n",
    "    # convert all numbers to float\n",
    "    table = table.replace('', np.nan, regex=True)\n",
    "    table[table.keys()[3:]] = table[table.keys()[3:]].astype(float)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_states_tables():\n",
    "    \"\"\"\n",
    "    Goes through the website 'http://www.wahlrecht.de/umfragen/laender.htm'\n",
    "    and extracts the table for states individually, \n",
    "    \n",
    "    Return: a dictionary containing the id names of the states as keywords and the \n",
    "            pd dataframes as values.\n",
    "    \"\"\"\n",
    "    tables = {} # {'state': df}\n",
    "\n",
    "    page = urllib.request.urlopen('http://www.wahlrecht.de/umfragen/laender.htm')\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Find the subtables\n",
    "    states = soup.find_all('th', colspan='10', id=True)\n",
    "    rows = soup.find_all('tr')\n",
    "    header = [col.get_text() for col in soup.find_all('th', class_=True, limit=9)]\n",
    "    \n",
    "    # Initialize with empty/unimportant values\n",
    "    table = [] # df\n",
    "    new_table = pd.DataFrame()\n",
    "    name = \"ignore\"\n",
    "    for row in rows:\n",
    "        # Start point of a new state\n",
    "        if row.find('th', colspan='10', id=True) != None:\n",
    "            table = []\n",
    "            name = row.contents[1].get('id')\n",
    "\n",
    "        # Read the data of the subtable\n",
    "        cols = row.find_all('td', rowspan=False)\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        table.append([ele if ele else None for ele in cols])\n",
    "        \n",
    "        # End point for each state\n",
    "        if row.find('th', colspan='10', class_=\"trenner\") != None: \n",
    "            # Don't use the information outside the states.\n",
    "            if name != 'ignore':\n",
    "                try:\n",
    "                    tables[name] = pd.DataFrame(table, columns=header)\n",
    "                except AssertionError:\n",
    "                    tables[name] = pd.DataFrame(columns=header)\n",
    "\n",
    "    # Add last table, that doesn't have trenner at the end\n",
    "    tables[name] = pd.DataFrame(table, columns=header)\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tables():\n",
    "    \"\"\"\n",
    "    extracts the tables from 'http://www.wahlrecht.de/umfragen/laender.htm',\n",
    "    cleans the tables,\n",
    "    returns a dictionary containing the abbreviations of the states as keywords\n",
    "    and the corresponding tables as values.\n",
    "    \"\"\"\n",
    "    states = get_states_tables()\n",
    "    for state, table in states.items():\n",
    "        states[state] = janitor(table)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tables = get_tables()\n",
    "# table = tables['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
